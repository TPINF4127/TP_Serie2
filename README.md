# INF4127 TPE_serie2 â€“ Descente de Gradient

## Description du projet


Ce dÃ©pÃ´t contient les travaux pratiques rÃ©alisÃ©s dans le cadre de l'unitÃ© **INF4127 : Optimisation II** du programme INF M1 Ã  l'UniversitÃ© de YaoundÃ© I.

## ğŸ¯ Objectif

Ã‰tudier et expÃ©rimenter des algorithmes d'optimisation numÃ©rique **sans contraintes** sur des fonctions de test multimodales. Ce TPE reprend les mÃ©thodes vues dans le support de cours (Chapitre 2, pages 32 Ã  34).

## ğŸ› ï¸ Fonctions de Test

Les expÃ©rimentations portent sur trois fonctions de deux variables \((x, y)\) :

1. **Rosenbrock **  
   \[
   f(x,y) = (1-x)^2 + 100(y-x^2)^2
   \]
2. **Quadratique**  
   \[
   f(x,y) = x^2 - y^2
   \]
3. **Himmelblau**  
   \[
   f(x,y) = (x^2 + y - 11)^2 + (x + y^2 - 7)^2
   \]

## âš™ï¸ Travail RÃ©alisÃ©

Pour chaque fonction, le notebook inclut :

- **Visualisation 3D** : Graphique de la surface pour identifier minima, maxima et points selles.
- **Calcul du gradient** : DÃ©rivation symbolique des fonctions.
- **ExpÃ©rimentation des algorithmes** : Descente de gradient, mÃ©thode de Newton, quasi-Newton.
- **Analyse des rÃ©sultats** :
  - Convergence selon le point de dÃ©part.
  - Vitesse de convergence (nombre d'itÃ©rations).
  - CapacitÃ© Ã  atteindre le minimum global.
  - Influence du learning rate ou de l'algorithme choisi.

## ğŸ§‘â€ğŸ’» Technologies

- **Langage** : Python 3.x  
- **BibliothÃ¨ques** : NumPy, SciPy, Matplotlib  
- **Support** : Jupyter Notebooks (.ipynb)

## ğŸ“… DÃ©lais

- **Date limite** : 23 Novembre 2025, 20h00  
- **Rendu** : Via ce dÃ©pÃ´t GitHub

## ğŸ‘¥ Auteurs

- Nangmo Feulfack Annick Duplesse (21S2530)    
- Nguefack Tangomo Chris Arthur (0000000)  

**Superviseur :** Pr. Paulin Melatagia

## ğŸ“œ Licence

Usage strictement acadÃ©mique et pÃ©dagogique.
